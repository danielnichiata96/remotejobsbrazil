# ü§ñ Sistema de Crawlers e Curadoria - Remote Jobs Brazil

## Vis√£o Geral

Este sistema implementa uma estrat√©gia h√≠brida para coletar e curar vagas remotas para desenvolvedores brasileiros, combinando automa√ß√£o inteligente com curadoria manual.

## ‚ú® Caracter√≠sticas Principais

### 1. **Crawlers Automatizados**
- **Fontes Suportadas**: Greenhouse, Lever, Ashby, Workable
- **Filtragem Inteligente**: Apenas vagas remotas que aceitam brasileiros
- **Sistema de Pontua√ß√£o**: 0-100 baseado em relev√¢ncia
- **Deduplica√ß√£o Autom√°tica**: Remove vagas repetidas mantendo a de maior score
- **Rate Limiting**: Respeita limites de API das fontes

### 2. **Sistema de Pontua√ß√£o**
```typescript
// Fatores de relev√¢ncia (pesos configur√°veis)
keywordWeights: {
  frontend: 8,      // React, JavaScript, TypeScript
  backend: 8,       // Node.js, Python, Java
  fullstack: 10,    // Full-stack positions (priorit√°rio)
  mobile: 7,        // React Native, Flutter
  devops: 6,        // AWS, Docker, Kubernetes
  data: 7,          // Data Science, ML
  levels: 3,        // Junior, Senior, Staff
  remote: 15,       // CR√çTICO - deve ser remoto
  brazilFriendly: 20 // CR√çTICO - deve aceitar brasileiros
}
```

### 3. **Curadoria Manual**
- **Interface Admin**: Dashboard completo para revis√£o
- **Bulk Operations**: Aprovar/rejeitar m√∫ltiplas vagas
- **Sistema de Status**: pending, approved, rejected, featured
- **Jobs Destacados**: Promo√ß√£o manual de oportunidades estrat√©gicas
- **Notas do Curador**: Coment√°rios internos para context

## üöÄ Como Iniciar

### Passo 1: Configura√ß√£o Inicial

```bash
# Execute o script de setup
./setup-crawlers.sh

# Configure as vari√°veis de ambiente em .env.local
ADMIN_KEY=sua-chave-admin-secreta
CRAWLER_RATE_LIMIT=10
CRAWLER_TIMEOUT=10000
```

### Passo 2: Migra√ß√µes do Banco (Supabase)

```sql
-- Execute a migra√ß√£o para adicionar campos de scoring
-- Arquivo: supabase/migrations/20250901120000_job-scoring-system.sql
```

### Passo 3: Acesso ao Admin

1. Acesse `/admin` para fazer login
2. Acesse `/admin/dashboard` para o dashboard completo
3. Configure crawlers e teste a coleta

## üì° API Endpoints

### `/api/crawlers` (GET)
Lista status de todos os crawlers configurados.

### `/api/crawlers` (POST)
Controla execu√ß√£o dos crawlers:

```javascript
// Executar todos os crawlers
{
  "action": "run_all"
}

// Executar crawler espec√≠fico
{
  "action": "run_specific",
  "crawlerName": "Stripe Greenhouse"
}

// Ativar/desativar crawler
{
  "action": "enable_crawler", // ou "disable_crawler"
  "crawlerName": "GitLab Greenhouse"
}
```

### `/api/jobs/bulk` (POST)
Opera√ß√µes em massa para curadoria:

```javascript
// Atualizar status de m√∫ltiplas vagas
{
  "action": "update_status",
  "jobIds": ["job-1", "job-2"],
  "status": "approved"
}
```

## üîß Configura√ß√£o de Crawlers

### Greenhouse
```typescript
{
  name: 'Remote.com Greenhouse',
  source: 'greenhouse',
  baseUrl: 'https://job-boards.greenhouse.io/remotecom',
  enabled: true,
  rateLimit: 10, // requests per minute
  timeout: 10000,
  maxPages: 3,
}

{
  name: 'GitLab Greenhouse',
  source: 'greenhouse',
  baseUrl: 'https://job-boards.greenhouse.io/gitlab',
  enabled: true,
  rateLimit: 10, // requests per minute
  timeout: 10000,
  maxPages: 5,
}
```

Observa√ß√£o: O crawler aceita tanto URLs de board `https://boards.greenhouse.io/<slug>` quanto `https://job-boards.greenhouse.io/<slug>`; internamente ele usa a API `https://boards-api.greenhouse.io/v1/boards/<slug>/jobs`.

### Adicionando Novas Fontes

1. Crie um novo crawler em `src/lib/crawlers/`
2. Estenda a classe `BaseCrawler`
3. Implemente os m√©todos de extra√ß√£o espec√≠ficos
4. Adicione √† configura√ß√£o no `manager.ts`

## üìà Estrat√©gia de Curadoria

### Fase 1: Coleta Automatizada
- Crawlers executam a cada 4 horas
- Filtram automaticamente por score > 30
- Removem duplicatas
- Status inicial: `pending`

### Fase 2: Revis√£o Manual
- Curador revisa vagas `pending`
- Aprova (`approved`) vagas relevantes
- Rejeita (`rejected`) vagas inadequadas
- Destaca (`featured`) oportunidades premium

### Fase 3: Publica√ß√£o
- Apenas vagas `approved` e `featured` aparecem no site
- Jobs `featured` aparecem primeiro na listagem
- Ordena√ß√£o por score + data de cria√ß√£o

## üõ† Manuten√ß√£o

### Script de Limpeza Autom√°tica
```bash
# Execute para remover duplicatas
node scripts/cleanup-duplicates.js
```

### Monitoramento via Logs
```bash
# Logs de execu√ß√£o dos crawlers
tail -f /var/log/crawler.log
```

### Cron Job Recomendado
```bash
# Executa crawlers a cada 4 horas
0 */4 * * * /path/to/project/scripts/run-crawlers.sh >> /var/log/crawler.log 2>&1

# Limpeza de duplicatas di√°ria
0 2 * * * cd /path/to/project && node scripts/cleanup-duplicates.js
```

## üìä M√©tricas e KPIs

### Dashboard Admin mostra:
- Total de crawlers ativos
- Jobs coletados por sess√£o
- Taxa de aprova√ß√£o por fonte
- Score m√©dio das vagas
- Duplicatas removidas
- Tempo desde √∫ltima execu√ß√£o

### Otimiza√ß√µes Futuras:
- Machine Learning para scoring autom√°tico
- Feedback loop baseado em aplica√ß√µes
- Analytics de performance por empresa
- Integra√ß√£o com mais fontes (AngelList, RemoteOK, etc.)

## üîç Troubleshooting

### Crawler Falha
1. Verifique logs no dashboard admin
2. Teste conectividade com a fonte
3. Verifique rate limits
4. Valide estrutura de resposta da API

### Duplicatas Persistentes
1. Execute script de limpeza manual
2. Ajuste algoritmo de detec√ß√£o
3. Verifique normaliza√ß√£o de texto

### Score Baixo Demais
1. Ajuste pesos no `scoring.ts`
2. Adicione keywords relevantes
3. Verifique crit√©rios de localiza√ß√£o

---

## üí° Pr√≥ximas Melhorias

1. **Integra√ß√£o com mais fontes**: AngelList, RemoteOK, We Work Remotely
2. **ML para scoring**: Usar hist√≥rico de aplica√ß√µes para melhorar relev√¢ncia
3. **Webhook notifications**: Alertas para novas vagas de alta qualidade
4. **API p√∫blica**: Permitir integra√ß√µes externas
5. **Analytics avan√ßado**: M√©tricas detalhadas de performance
